# Data

## Description

We are basing our analysis on two datasets, a dataset of taxi use in New
York and historical weather data. For both datasets, we will limit our
analysis to 2019-01-01 00:00:00 – 2024-06-25 23:00:00, in the interest of trying to catch about a
year of pre-pandemic norms to help interpret pandemic and post-pandemic use
of taxis. The taxi data only go to August, but the weather data only go
through most of June this year.
We are basing our analysis on two datasets, a dataset of taxi use in New York and historical weather data. For both datasets, we will limit our analysis to 2019–August 2024, in the interest of trying to catch about a year of pre-pandemic norms to help interpret pandemic and post-pandemic use of taxis.

The taxi data are provided by New York’s [Taxi and Limousine
Commission](https://www.nyc.gov/site/tlc/index.page). They [provide taxi
data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page), in
[parquet](https://parquet.apache.org/) format, going back to 2009. Since
February of 2019, they have also included data on trips serviced by
companies like Lyft and Uber, which are classified as “high-volume for-hire
vehicle” (HVFHV) trips. The data are updated monthly, with a two-month lag.
Our focus is on yellow taxi and HVFHV trips, because our focus
is on intra-Manhattan trips. Only yellow cabs can pick up passengers in
most of Manhattan, so we are ignoring green cabs and regular for-hire
vehicles (town cars and limousines). Yellow cab data have [19
columns](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf),
and FHVHV data have [24 columns](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf).
With our Python scripts, we consolidate the data to create 
aggregated hourly statistics on trip duration, trip distance, fare amount,
and tip amount.
The taxi data are provided by New York’s [Taxi and Limousine Commission](https://www.nyc.gov/site/tlc/index.page). They [provide taxi data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page), in [parquet](https://parquet.apache.org/) format, going back to 2009. Since February of 2019 they have also included data on trips serviced by companies like Lyft and Uber, which are classified as “high-volume for-hire vehicle” (HVFHV) trips. The data are updated monthly, with a two-month lag; however, at the time of submission, only data through August 2024 were available. Our focus is on yellow taxi and HVFHV trips, because our focus is on intra-Manhattan trips. Only yellow cabs can pick up passengers in most of Manhattan, so we are ignoring green cabs and regular for-hire vehicles (town cars and limousines). Yellow cab data have [19 columns](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf), of which the pertient columns for us are pickup/dropoff date and time (`tpep_pickup_datetime`/`tpep_dropoff_datetime` in a datetime format), the pickup/dropoff locations (`PULocationID`/`DOLocationID` in integer format, corresponding to [NYC taxi zones](https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv)) in order to filter on Manhattan-only trips, passenger count (`Passenger_count` in integer format), fare amount (`Fare_amount` in integer format), and tip amount (`Tip_amount` in integer format).

The historical weather data are provided by the [Global Historical Climate
Network hourly
(GHCNh)](https://www.ncei.noaa.gov/products/global-historical-climatology-network-hourly),
which provides hourly weather data going back over two centuries for New
York City. The data come in
[annual parquet files for download by
station](https://www.ncei.noaa.gov/oa/global-historical-climatology-network/index.html#hourly/access/by-year/).
Our station, `KNYC0`, is listed in the GHCNh as `USW00094728`, and it is
the weather station in Central Park. The data come in over 200 columns to
account for the variability that can occur in the terse
[METAR](https://en.wikipedia.org/wiki/METAR) report for airplanes, which is
also included under `remarks`. The government provides a
[codebook](https://www.ncei.noaa.gov/oa/global-historical-climatology-network/hourly/doc/ghcnh_DOCUMENTATION.pdf) to describe the remaining data. As we are interested in what conditions
determine a “nice” day for not using a taxi, we want to keep as much data
from the weather report as possible, including temperature, precipitation,
sky cover, snow, humidity, and so on. 
The historical weather data are provided by the [Global Historical Climate Network hourly (GHCNh)](https://www.ncei.noaa.gov/products/global-historical-climatology-network-hourly), which provides hourly weather data going back over two centuries for New York City. We will be limiting ourselves to 2019–present, which comes in [annual parquet files for download by station](https://www.ncei.noaa.gov/oa/global-historical-climatology-network/index.html#hourly/access/by-year/). Our station, `KNYC0`, is listed in the GHCNh as `USW00094728`, and it is the weather station in Central Park. The data come in over 200 columns to account for the variability that can occur in the terse [METAR](https://en.wikipedia.org/wiki/METAR) report for airplanes, which is also included under `remarks`. The government provides a [codebook](https://www.ncei.noaa.gov/oa/global-historical-climatology-network/hourly/doc/ghcnh_DOCUMENTATION.pdf). We may simply feed the reports to the [python-metar](https://github.com/python-metar/python-metar) library, which parses the reports for us. As we are interested in what conditions determine a “nice” day for not using a taxi, we want to keep as much data from the weather report as possible, including temperature, precipitation, sky cover, snow, humidity, and so on.

## Missing value analysis

The taxi data are notoriously (as in, persistently) messy, registering
trips outside the bounds of the asserted date and giving results that seem
extremely unlikely, like large negative tips or negative trip durations. 
To counter these anomalies, we are liming ourselves to trips between one
minute and two hours long, and, when consolidating monthly data to yearly
data, we filter out all results from other years.
In
the aggregate, however, the outliers generally wash out, as we have a
record of over 500 million yellow cab and Uber/Lyft rides. Nevertheless, there
is one data point where no trips are recorded: 2:00am for March 10, 2019.
However, there are data on both sides, so we will interpolate results for
this time. Who knows what happened to the taxi system that affected both
Uber and yellow taxis. Additionally, the tip amounts for Uber/Lyft are
almost certainly incorrect, as over 75% of rides report no tip at all. As
such, we will drop the tip and fare amounts from our data to account for
this. We had suspected that a higher tip percentage might be related to a
nice day, even though we assume taxi usage is lower, but the data are simply
unreliable.

For weather, we have a large array of missing values, but the `remarks`
column is missing only 36 entries, for a general station uptime of 99.93%.
Many of the columns have many more missing values, but that is because the
way the weather works is by reporting a `NaN` for the absence of data. For
example, if there are no clouds in the sky, the sky cover values will be
`NaN`, not something like "Clear." That said, we have a consecutive period
of 24 hours’ worth of missing data across May 31, 2023 to June 1, 2023.
This includes missing remarks, suggesting the station was down. In our
imagination, a peregrine falcon ate the station.

The other 12 missing remarks are scattered across the dataset.

Because we have a total of 48072 points in time in our dataset, it is hard
to see any of the missing data in any plot that includes the entire
stretch.

(suggested: 2 graphs plus commentary)


```{r}
#| message: false
#| warning: false
library(ggplot2)
library(arrow)
library(dplyr)
library(lubridate)
library(scales)
library(tidyr)
df <- read_parquet("data/complete_weather_and_taxi_data.parquet")
```
```{r}
#| fig-height: 3
#| message: false
heatmap_data <- df |>
    mutate(week_start=lubridate::floor_date(date, unit="week"),
        week = lubridate::week(date),
        year = lubridate::year(date)) |>
    group_by(year, week) |>
    summarize(n_missing=sum(is.na(temperature)))

ggplot(heatmap_data, aes(x = week, y = year, fill = n_missing)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "red", name = "Missing Count") +
  scale_x_continuous(breaks = seq(1, 52, by = 4)) + 
  scale_y_continuous(breaks = seq(2019, 2024, by=1))+
  labs(
    title = "Number of missing temperature observations by date",
    x = "Week",
    y = "Year"
  ) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
#| fig-height: 6
#| fig-width: 8
library(redav)
df |>
  select(-c(pres_wx_AU3, snow_depth, pres_wx_AU2, sky_cover_2, wind_direction, sky_cover_3, pres_wx_AU1, wind_gust, juno, via)) |>
  plot_missing(percent=FALSE, max_rows=10, max_cols=10, num_char=7)